# DinarAI

DinarAI — это интеллектуальный API-сервис, специализирующийся на обработке запросов по исламскому финансированию с использованием современных технологий искусственного интеллекта.

## Описание проекта

Проект представляет собой FastAPI приложение, которое обрабатывает пользовательские запросы, связанные с исламским финансированием. Система использует современные языковые модели для генерации ответов на основе проверенных источников информации, включая как локальную векторную базу данных, так и результаты веб-поиска.

## Основные компоненты

- **Assistant** — центральный компонент, координирующий обработку запросов
- **Classify Service** — классифицирует запросы по типу (разговорные, тематические, требующие веб-поиска)
- **Context Service** — управляет историей диалога и контекстом общения
- **LLM Service** — генерирует ответы с использованием языковых моделей
- **Vector Store Service** — осуществляет поиск по векторной базе данных
- **Web Search Service** — выполняет поиск актуальной информации в интернете

## Архитектура системы

1. Входящий запрос проходит через несколько этапов классификации:
   - Определение, является ли запрос разговорным
   - Проверка на соответствие тематике исламского финансирования
   - Определение необходимости веб-поиска для ответа

2. В зависимости от результатов классификации:
   - Для разговорных запросов генерируется ответ на основе шаблона
   - Для тематических запросов выполняется поиск в векторной базе данных
   - Для запросов, требующих актуальной информации, выполняется веб-поиск

3. Система генерирует ответ на основе найденной информации и истории диалога

4. Ответ и источники возвращаются пользователю

## Технологии

- **FastAPI** — высокопроизводительный веб-фреймворк для создания API
- **Redis** - это СУБД, которая хранит информацию в виде пар ключ-значение. Используется для хранения истории диалога.
- **LangChain** — фреймворк для работы с языковыми моделями
- **ChromaDB** — векторная база данных для хранения и поиска информации
- **Mistral AI/OpenRouter** — языковые модели для генерации ответов
- **Tavily API** — сервис для веб-поиска

## Установка

1. Клонируйте репозиторий
2. Установите зависимости:
```bash
pip install -r src/requirements.txt
```
3. Создайте файл `.env` на основе `.env-example`:
```bash
MISTRAL_API_KEY=your_key
OPENROUTER_API_KEY=your_key
TAVILY_API_KEY=your_key
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
```

## Запуск

```bash
cd src
uvicorn main:app --reload
```

После запуска API будет доступен по адресу: `http://localhost:8000`

## API Endpoints

### GET /
Приветственное сообщение и проверка работоспособности API.

### POST /chat
Основной эндпоинт для обработки запросов пользователей.

Пример запроса:
```json
{
    "chat_id": 123,
    "question": "Что такое мурабаха и какие банки в России предлагают услуги на основе этого инструмента?"
}
```

Пример ответа:
```json
{
    "answer": "Подробный ответ о мурабахе...",
    "source_text": ["Источник 1", "Источник 2"],
    "urls": ["https://example.com/1", "https://example.com/2"]
}
```

## Логирование

Система использует встроенный логгер для отслеживания всех операций и возможных ошибок.

## Структура проекта

```
src/
├── assistant.py         # Основной класс для обработки запросов
├── chroma/              # Директория для векторной базы данных
├── config/              # Конфигурационные файлы
├── logger.py            # Настройка логирования
├── main.py              # Точка входа FastAPI приложения
├── requirements.txt     # Зависимости проекта
├── schemas/             # Pydantic модели для валидации данных
└── services/            # Сервисы для обработки запросов
    ├── classify_service.py    # Классификация запросов
    ├── context_service.py     # Управление контекстом диалога
    ├── llm_service.py         # Работа с языковыми моделями
    ├── vector_store_service.py # Работа с векторной базой данных
    └── web_search_service.py  # Веб-поиск
```
